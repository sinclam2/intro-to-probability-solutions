{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Expected value is only useful when the possible outcomes don't deviate too much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# *Variance*\n",
    "\n",
    "# Definition 6.3\n",
    "\n",
    "### Let $X$ be a numerically valued random variable with expected value $E(X) = \\mu$\n",
    "\n",
    "### Then, the *variance* of $X$, denoted by $V(X)$ is:\n",
    "\n",
    "# $V(X) = E((X-\\mu)^{2})$\n",
    "\n",
    "### Using Theorem 6.1, this becomes:\n",
    "\n",
    "# $V(X) =\\sum_{x}(x-\\mu)^{2}m(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# *Standard Deviation*\n",
    "\n",
    "### The *standard deciation* of $X$ is just the square root of the variance i.e.:\n",
    "\n",
    "# $D(X) = \\sqrt{V(X)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is common to use $\\sigma$ to represent standard deviation and $\\sigma^{2}$ to represent variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Example\n",
    "\n",
    "- Consider the roll of a die\n",
    "\n",
    "- Let $X$ be the number of a roll. Then:\n",
    "\n",
    "# $\\mu = (1)\\frac{1}{6} + (2)\\frac{1}{6} + (3)\\frac{1}{6} + (4)\\frac{1}{6} + (5)\\frac{1}{6} + (6)\\frac{1}{6} = \\frac{7}{2}$\n",
    "\n",
    "### From this, we calculate $\\sigma^{2}$\n",
    "\n",
    "### $\\sigma^{2} = (1-\\frac{7}{2})^{2}\\cdot\\frac{1}{6} + (2-\\frac{7}{2})^{2}\\cdot\\frac{1}{6} + (3-\\frac{7}{2})^{2}\\cdot\\frac{1}{6} + (4-\\frac{7}{2})^{2}\\cdot\\frac{1}{6} + (5-\\frac{7}{2})^{2}\\cdot\\frac{1}{6} + (6-\\frac{7}{2})^{2}\\cdot\\frac{1}{6}$\n",
    "\n",
    "### $ = \\frac{1}{6}\\left ( \\frac{25}{4} + \\frac{9}{4} + \\frac{1}{4} + \\frac{1}{4} + \\frac{9}{4} + \\frac{25}{4} \\right ) = \\frac{35}{12}$\n",
    "\n",
    "### And then the standard deviation is simply $\\sigma = \\sqrt{\\frac{35}{12}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Calculation of Variance*\n",
    "\n",
    "# Theorem 6.6\n",
    "\n",
    "### If $X$ is a random variable with $E(X) = \\mu$ then:\n",
    "\n",
    "# $V(X) = E(X^{2}) - \\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Example (again)\n",
    "\n",
    "- This time we can use Theorem 6.6 to calculate the variance\n",
    "\n",
    "### $E(X^{2}) = (1^{2})\\frac{1}{6} + (2^{2})\\frac{1}{6} + (3^{2})\\frac{1}{6} + (4^{2})\\frac{1}{6} + (5^{2})\\frac{1}{6} + (6^{2})\\frac{1}{6} = \\frac{1^{2}+2^{2}+3^{2}+4^{2}+5^{2}+6^{2}}{6} = \\frac{91}{6}$\n",
    "\n",
    "### $\\implies \\sigma^{2} = \\frac{91}{6} - (\\frac{7}{2})^{2} = \\frac{35}{12}$\n",
    "\n",
    "**Got the same answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# *Properties of Variance*\n",
    "\n",
    "# Theorem 6.7\n",
    "\n",
    "### If $X$ is a random variable and $c$ is a constant, then:\n",
    "\n",
    "# $V(cX) = c^{2}V(X)$ ; $V(X+c) = V(X)$\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "# Theorem 6.8\n",
    "\n",
    "### Let $X$ and $Y$ be two independent random variables. Then:\n",
    "\n",
    "# $V(X+Y) = V(X) + V(Y)$\n",
    "\n",
    "### i.e. the variance for the sum of two random variables is the sum of the variances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Theorem 6.9\n",
    "\n",
    "### Let $X_{1}$, $X_{2}$, ...,$X_{n}$ be $n$ independent trials with the same expected value $\\mu$ and the same variance $\\sigma^{2}$\n",
    "\n",
    "### Let $S_{n}$ be the random variable representing the *sum* of $X_{1}$, $X_{2}$, ...,$X_{n}$\n",
    "\n",
    "### Let $A_{n}$ be the random variable representing the *mean* of $X_{1}$, $X_{2}$, ...,$X_{n}$\n",
    "\n",
    "### Then:\n",
    "\n",
    "### $E(S_{n}) = E(X_{1}+X_{2}+...+X_{n}) = E(X_{1})+E(X_{2})+...+E(X_{n}) = \\mu + \\mu + ... + \\mu = n\\cdot\\mu$\n",
    "\n",
    "### $E(A_{n}) = E(\\frac{S_{n}}{n}) = E(\\frac{1}{n}\\cdot S_{n}) = \\frac{1}{n}\\cdot n \\cdot \\mu = \\mu$\n",
    "\n",
    "### $V(S_{n}) = V(X_{1}+X_{2}+...+X_{n}) + V(X_{1})+V(X_{2})+...+V(X_{n}) = n\\cdot \\sigma^{2}$\n",
    "\n",
    "### $V(A_{n}) = V(\\frac{S_{n}}{n}) = V(\\frac{1}{n}\\cdot S_{n}) = \\frac{1}{n^{2}}\\cdot n \\cdot \\sigma^{n} = \\frac{\\sigma^{2}}{n}$\n",
    "\n",
    "### $D(S_{n}) = \\sqrt{n}\\cdot \\sigma$\n",
    "\n",
    "### $D(A_{n}) = \\frac{\\sigma}{\\sqrt{n}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "# *Bernoulli Trials*\n",
    "\n",
    "### Let $X_{i}$  be the random variable representing the sucess or failure of a Bernoulli trial (1 implies success, 0 failure)\n",
    "\n",
    "### Then $E(X_{i}) = 1\\cdot p + 0\\cdot(1-p) = p$ and $E(X_{i}^{2}) = (1^{2})\\cdot p + (0^{2})\\cdot(1-p) = p$\n",
    "\n",
    "### Therefore $V(X_{i}) = E(X_{i}^{2}) - (E(X_{i}))^{2} = p - p^{2} = p(1-p) = pq$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# *Binomial Distribution*\n",
    "\n",
    "### Recall: the binomial distribution represents a serious of Bernoulli trials\n",
    "\n",
    "### This means we can let $S_{n}$ be the random variable representing the number of successes in a series of $n$ Bernoulli trials, and let $A_{n} = \\frac{S_{n}}{n}$ represent the probability\n",
    "\n",
    "### $S_{n} = X_{1}+X_{2}+...+X_{n}$\n",
    "\n",
    "### $\\implies E(S_{n}) = E(X_{1}+X_{2}+...+X_{n}) = E(X_{1})+E(X_{2})+...+E(X_{n}) = n\\cdot p$\n",
    "\n",
    "### From Theorem 6.8: $V(S_{n}) = V(X_{1}+X_{2}+...+X_{n}) = V(X_{1})+V(X_{2})+...+V(X_{n}) = n\\cdot p \\cdot q$\n",
    "\n",
    "\n",
    "### $\\implies E(A_{n}) = p$ and $V(A_{n}) = V(\\frac{1}{n}\\cdot S_{n}) = \\frac{1}{n^{2}}\\cdot n\\cdot p \\cdot q = \\frac{pq}{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "## Example\n",
    "\n",
    "### Let $T$ represent the number of Bernoulli trials until the first success\n",
    "\n",
    "### We know right away that this is geometrically distributed: $P(T=k) = q^{k}p$\n",
    "\n",
    "### This implies $E(T) = (1)\\cdot p + (2)\\cdot q\\cdot p + ... = \\sum_{i=1}^{\\infty}i\\cdot q^{i-1} \\cdot p = \\frac{p}{(1-q)^{2}} = \\frac{1}{p}$\n",
    "\n",
    "### $\\implies E(T^{2}) = p\\frac{1+q}{(1-q)^{3}} = \\frac{1+q}{p^{2}} \\implies V(T) = E(T^{2}) - E(T)^{2} = \\frac{1+q}{p^{2}} - \\frac{1}{p^{2}} = \\frac{q}{p^{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# *Poisson Distribution*\n",
    "\n",
    "### Recall: The poisson distribution is an approximation for the binomial distribution where $n$ is very large and $p$ is very small\n",
    "\n",
    "### $P(X=k) = e^{-\\lambda}\\frac{\\lambda^{k}}{k!}$\n",
    "\n",
    "\n",
    "### We just showed that for the binomial distribution $V(S_{n}) = npq$, and we know that $\\lambda = np$\n",
    "\n",
    "### But since $p$ is very small, $q$ must be very close to 1 so for a random variable $X$ with poisson dist'n: $V(X) = \\lambda (q) = \\lambda$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
